tweets
p
p<- predict(fit, newdata = train)
p
c(1, 2)
data
data <- c('Yo apoyo a laso, vamos por el cambio.', 'Yo apyo a lenin, vamos por mas.')
data
tweets = read.csv("extraccioniaLasso2b.csv", sep = "," , encoding="UTF-8", stringsAsFactors=FALSE, header=F)$tweet
tweets
tweets = read.csv("extraccioniaLasso2b.csv", header=F)$tweet
data2 <- c('Yo apoyo a laso, vamos por el cambio.', 'Yo apyo a lenin, vamos por mas.')
tweets
tweets = readLines("extraccioniaLasso2b.csv", header=F)$tweet
dat <- read.csv("extraccioniaLasso2b.csv",header=F)
library(caret)
dat
dat <- read.csv("extraccioniaLasso2b.csv",header=F)$V6
dat
data2
tweets
tweets <- read.csv("extraccioniaLasso2b.csv",header=F)$V6
tweets
list(tweets)
tweets <- c(read.csv("extraccioniaLasso2b.csv",header=F)$V6)
tweets
tweets
fit <- train(y ~ ., data = train, method = "svmRadial", trControl= tc)
data <- c('Yo apoyo a laso, vamos por el cambio.', 'Yo apoyo a lenin, vamos por mas.')
corpus <- VCorpus(VectorSource(data))
tdm <- DocumentTermMatrix(corpus, list(removePunctuation = TRUE, stopwords = TRUE, stemming = TRUE, removeNumbers = TRUE))
train <- as.matrix(tdm)
train <- cbind(train, c(1, 2))
colnames(train)[ncol(train)] <- 'y'
train <- as.data.frame(train)
train$y <- as.factor(train$y)
fit <- train(y ~ ., data = train, method = "svmRadial", trControl= tc)
fit <- train(y ~ ., data = train, method = "svmRadial")
fit <- train(y ~ ., data = train, method = "svmLinear")
warnings()
predictSVM
fit
tweets = read.csv("extraccioniaLasso2b.csv", sep = "," , encoding="UTF-8", stringsAsFactors=FALSE)
table(tweets$sentimiento)
corpus = Corpus(VectorSource(tweets$tweet))
print(corpus)
length(corpus)
content(corpus[[12]])
content(corpus[[1500]])
corpus = tm_map(corpus, tolower) # texto a minusculas
corpus = tm_map(corpus, PlainTextDocument) #
corpus <- tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, c(stopwords("es"),"elecciones2017ec"))
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, stemDocument, language="es")
content(corpus[[12]])
content(corpus[[1500]])
frequencies = DocumentTermMatrix(corpus)
frequencies
#inspect(frequencies[50:60, 100:200])
# Terminos mas frecuentes
findFreqTerms(frequencies, lowfreq = 50)
# Eliminar palabras poco repetidas
frequencies
sparse <- removeSparseTerms(frequencies, sparse = 0.995)
sparse
findFreqTerms(sparse, lowfreq = 70)
tweetsSparse <- as.data.frame(as.matrix(sparse))
colnames(tweetsSparse) = make.names(colnames(tweetsSparse))
tweetsSparse$sentimiento <- tweets$sentimiento
set.seed(12)
split <- sample.split(tweetsSparse$sentimiento, SplitRatio = 0.8)
trainSparse = subset(tweetsSparse, split=TRUE)
testSparse = subset(tweetsSparse, split=FALSE)
table(testSparse$sentimiento)
# Evaluar el modelo
# Entrenar modelo
SVM <- svm(as.factor(sentimiento)~ ., data = trainSparse)
summary(SVM)
trainSparse
predictSVM <- predict(SVM, newdata = testSparse)
table(predictSVM)
confusionMatrix(predictSVM, testSparse$sentimiento)
t
data2 <- c("Yo apoyo a lenin y a la revolucion")
corpus <- VCorpus(VectorSource(data2))
tdm <- DocumentTermMatrix(corpus, control = list(dictionary = Terms(frequencies), removePunctuation = TRUE, stopwords = TRUE, stemming = TRUE, removeNumbers = TRUE))
test <- as.matrix(tdm)
predict(SVM, newdata = test)
frequencies
test
predict(SVM, newdata = test)
predictSVM
test
data <- c('Yo apoyo a laso, vamos por el cambio.', 'Yo apoyo a lenin, vamos por mas.')
corpus <- VCorpus(VectorSource(data))
tdm <- DocumentTermMatrix(corpus, list(removePunctuation = TRUE, stopwords = TRUE, stemming = TRUE, removeNumbers = TRUE))
train <- as.matrix(tdm)
train <- cbind(train, c(1, 2))
colnames(train)[ncol(train)] <- 'y'
train <- as.data.frame(train)
train$y <- as.factor(train$y)
fit <- train(y ~ ., data = train, method = 'bayesglm')
warnings()
p<- predict(fit, newdata = train)
confusionMatrix(p, c(1, 2))
data2 <- c('Voy a votar por lenin para ir por mas')
corpus <- VCorpus(VectorSource(data2))
tdm <- DocumentTermMatrix(corpus, control = list(dictionary = Terms(tdm), removePunctuation = TRUE, stopwords = TRUE, stemming = TRUE, removeNumbers = TRUE))
test <- as.matrix(tdm)
predict(fit, newdata = test)
tdm
corpus
data2
corpus
control
tdm
frequencies
tweets = read.csv("extraccioniaLasso2b.csv", sep = "," , encoding="UTF-8", stringsAsFactors=FALSE)
table(tweets$sentimiento)
corpus = Corpus(VectorSource(tweets$tweet))
print(corpus)
length(corpus)
content(corpus[[12]])
content(corpus[[1500]])
corpus = tm_map(corpus, tolower) # texto a minusculas
corpus = tm_map(corpus, PlainTextDocument) #
corpus <- tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, c(stopwords("es"),"elecciones2017ec"))
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, stemDocument, language="es")
content(corpus[[12]])
content(corpus[[1500]])
frequencies = DocumentTermMatrix(corpus)
frequencies
#inspect(frequencies[50:60, 100:200])
# Terminos mas frecuentes
findFreqTerms(frequencies, lowfreq = 50)
# Eliminar palabras poco repetidas
frequencies
sparse <- removeSparseTerms(frequencies, sparse = 0.995)
sparse
findFreqTerms(sparse, lowfreq = 70)
tweetsSparse <- as.data.frame(as.matrix(sparse))
colnames(tweetsSparse) = make.names(colnames(tweetsSparse))
tweetsSparse$sentimiento <- tweets$sentimiento
set.seed(12)
split <- sample.split(tweetsSparse$sentimiento, SplitRatio = 0.8)
trainSparse = subset(tweetsSparse, split=TRUE)
testSparse = subset(tweetsSparse, split=FALSE)
table(testSparse$sentimiento)
# Evaluar el modelo
# Entrenar modelo
SVM <- svm(as.factor(sentimiento)~ ., data = trainSparse)
summary(SVM)
trainSparse
predictSVM <- predict(SVM, newdata = testSparse)
table(predictSVM)
confusionMatrix(predictSVM, testSparse$sentimiento)
frequencies
tdm <- DocumentTermMatrix(corpus, list(removePunctuation = TRUE, stopwords = TRUE, stemming = TRUE, removeNumbers = TRUE))
data <- c('Yo apoyo a laso, vamos por el cambio.', 'Yo apoyo a lenin, vamos por mas.')
corpus <- VCorpus(VectorSource(data))
tdm <- DocumentTermMatrix(corpus, list(removePunctuation = TRUE, stopwords = TRUE, stemming = TRUE, removeNumbers = TRUE))
train <- as.matrix(tdm)
train <- cbind(train, c(1, 2))
colnames(train)[ncol(train)] <- 'y'
train <- as.data.frame(train)
train$y <- as.factor(train$y)
fit <- train(y ~ ., data = train, method = 'bayesglm')
warnings()
p<- predict(fit, newdata = train)
confusionMatrix(p, c(1, 2))
data2 <- c('Voy a votar por lenin para ir por mas')
corpus <- VCorpus(VectorSource(data2))
tdm <- DocumentTermMatrix(corpus, control = list(dictionary = Terms(tdm), removePunctuation = TRUE, stopwords = TRUE, stemming = TRUE, removeNumbers = TRUE))
test <- as.matrix(tdm)
predict(fit, newdata = test)
tdm
tdm <- DocumentTermMatrix(corpus, list(removePunctuation = TRUE, stopwords = TRUE, stemming = TRUE, removeNumbers = TRUE))
tdm
table(tdm)
corpus
data
tweets = read.csv("extraccioniaLasso2b.csv", sep = "," , encoding="UTF-8", stringsAsFactors=FALSE)
tweets2 = read.csv("extraccioniaLasso2b.csv", sep = "," , encoding="UTF-8", stringsAsFactors=FALSE)
table(tweets$sentimiento)
corpus = Corpus(VectorSource(tweets$tweet))
corpus2 = Corpus(VectorSource(tweets2$tweet))
print(corpus)
length(corpus)
content(corpus[[12]])
content(corpus[[1500]])
corpus = tm_map(corpus, tolower) # texto a minusculas
corpus = tm_map(corpus, PlainTextDocument) #
corpus <- tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, c(stopwords("es"),"elecciones2017ec"))
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, stemDocument, language="es")
corpus2 = tm_map(corpus2, tolower) # texto a minusculas
corpus2 = tm_map(corpus2, PlainTextDocument) #
corpus2 <- tm_map(corpus2, removePunctuation)
corpus2 = tm_map(corpus2, removeWords, c(stopwords("es"),"elecciones2017ec"))
corpus2 = tm_map(corpus2, stripWhitespace)
corpus2 = tm_map(corpus2, stemDocument, language="es")
content(corpus[[12]])
content(corpus[[1500]])
frequencies = DocumentTermMatrix(corpus)
frequencies2 = DocumentTermMatrix(corpus2)
frequencies
findFreqTerms(frequencies, lowfreq = 50)
frequencies
sparse <- removeSparseTerms(frequencies, sparse = 0.995)
sparse2 <- removeSparseTerms(frequencies, sparse = 0.995)
sparse
findFreqTerms(sparse, lowfreq = 70)
tweetsSparse <- as.data.frame(as.matrix(sparse))
tweetsSparse2 <- as.data.frame(as.matrix(sparse2))
colnames(tweetsSparse) = make.names(colnames(tweetsSparse))
colnames(tweetsSparse2) = make.names(colnames(tweetsSparse2))
tweetsSparse$sentimiento <- tweets$sentimiento
tweetsSparse2$sentimiento <- tweets2$sentimiento
set.seed(12)
split <- sample.split(tweetsSparse$sentimiento, SplitRatio = 0.8)
split2 <- sample.split(tweetsSparse2$sentimiento, SplitRatio = 0.8)
trainSparse = subset(tweetsSparse, split=TRUE)
testSparse = subset(tweetsSparse, split=FALSE)
trainSparse2 = subset(tweetsSparse2, split=TRUE)
testSparse2 = subset(tweetsSparse2, split=FALSE)
table(testSparse$sentimiento)
SVM <- svm(as.factor(sentimiento)~ ., data = trainSparse)
summary(SVM)
predictSVM <- predict(SVM, newdata = testSparse)
table(predictSVM)
confusionMatrix(predictSVM, testSparse$sentimiento)
predictSVM <- predict(SVM, newdata = testSparse2)
predictSVM2 <- predict(SVM, newdata = testSparse2)
predictSVM2
table(predictSVM2)
tdm
data2 <- c('Voy a votar por lenin para ir por mas')
corpus <- VCorpus(VectorSource(data2))
tdm <- DocumentTermMatrix(corpus, control = list(dictionary = Terms(tweetsSparse), removePunctuation = TRUE, stopwords = TRUE, stemming = TRUE, removeNumbers = TRUE))
tdm <- DocumentTermMatrix(corpus, control = list(dictionary = tweetsSparse, removePunctuation = TRUE, stopwords = TRUE, stemming = TRUE, removeNumbers = TRUE))
test <- as.matrix(tdm)
predict(fit, newdata = test)
test
predict(SVM, newdata = test)
tweets = read.csv("extraccioniaLasso2b.csv", sep = "," , encoding="UTF-8", stringsAsFactors=FALSE)
table(tweets$sentimiento)
corpus = Corpus(VectorSource(tweets$tweet))
corpus = tm_map(corpus, tolower) # texto a minusculas
corpus = tm_map(corpus, PlainTextDocument) #
corpus <- tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, c(stopwords("es"),"elecciones2017ec"))
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, stemDocument, language="es")
frequencies = DocumentTermMatrix(corpus)
# Eliminar palabras poco repetidas
sparse <- removeSparseTerms(frequencies, sparse = 0.995)
tweetsSparse <- as.data.frame(as.matrix(sparse))
colnames(tweetsSparse) = make.names(colnames(tweetsSparse))
tweetsSparse$sentimiento <- tweets$sentimiento
set.seed(12)
split <- sample.split(tweetsSparse$sentimiento, SplitRatio = 0.8)
trainSparse = subset(tweetsSparse, split=TRUE)
testSparse = subset(tweetsSparse, split=FALSE)
table(testSparse$sentimiento)
# Evaluar el modelo
# Entrenar modelo
SVM <- svm(as.factor(sentimiento)~ ., data = trainSparse)
summary(SVM)
trainSparse
predictSVM <- predict(SVM, newdata = testSparse)
table(predictSVM)
confusionMatrix(predictSVM, testSparse$sentimiento)
# Test data.
data2 <- c('Voy a votar por lenin para ir por mas')
corpus <- VCorpus(VectorSource(data2))
tdm <- DocumentTermMatrix(corpus, control = list(dictionary = tweetsSparse, removePunctuation = TRUE, stopwords = TRUE, stemming = TRUE, removeNumbers = TRUE))
test <- as.matrix(tdm)
predict(SVM, newdata = test)
corpus <- VectorSource(data2)
tdm <- DocumentTermMatrix(corpus, control = list(dictionary = tweetsSparse, removePunctuation = TRUE, stopwords = TRUE, stemming = TRUE, removeNumbers = TRUE))
corpus <- VCorpus(VectorSource(data2))
tdm <- DocumentTermMatrix(corpus, control = list(dictionary = tweetsSparse, removePunctuation = TRUE, stopwords = TRUE, stemming = TRUE, removeNumbers = TRUE))
test <- as.matrix(tdm)
predict(SVM, newdata = test)
View(test)
View(test)
tdm <- DocumentTermMatrix(corpus, control = list(dictionary = tweetsSparse, removePunctuation = TRUE, stopwords = TRUE, stemming = TRUE, removeNumbers = TRUE))
predict(SVM, newdata = tdm)
sparse <- removeSparseTerms(tdm, sparse = 0.995)
sparse
sparse
sparse <- removeSparseTerms(frequencies, sparse = 0.995)
sparse
tdm <- DocumentTermMatrix(corpus, control = list(dictionary = tweetsSparse, removePunctuation = TRUE, stopwords = TRUE, stemming = TRUE, removeNumbers = TRUE))
sparse <- removeSparseTerms(tdm, sparse = 0.995)
sparse
tdm
tdm
frequencies
tdm <- DocumentTermMatrix(corpus, control = list(dictionary = frequencies, removePunctuation = TRUE, stopwords = TRUE, stemming = TRUE, removeNumbers = TRUE))
tdm <- DocumentTermMatrix(corpus, control = list(dictionary = tweetsSparse, removePunctuation = TRUE, stopwords = TRUE, stemming = TRUE, removeNumbers = TRUE))
test <- as.matrix(tdm)
predict(SVM, newdata = test)
library(e1071)
x <- c(1:10)
y <- c(0,0,0,0,1,0,1,1,1,1)
test <- c(11:15)
mod <- svm(y ~ x, kernel = "linear", gamma = 1, cost = 2, type="C-classification")
predict(mod, newdata = test)
trainSparse
trainSparse
trainSparse = subset(tweetsSparse[0:100])
trainSparse
x = tweetsSparse[1:1200,]
x
x = tweetsSparse[1:100,]
x
x = tweetsSparse[1:50,]
x
x = tweetsSparse[1:1721,]
x
x = tweetsSparse[1:1722,]
x
x = tweetsSparse[1:1723,]
x
x = tweetsSparse[1:5000,]
split <- sample.split(tweetsSparse$sentimiento, SplitRatio = 0.8)
x
x = tweetsSparse[4000:5000,]
x
testSparse
testSparse = tweetsSparse[1721:4645]
testSparse = tweetsSparse[1721:4645,]
testSparse
library(stringr)
library(tm) # text mining
library(SnowballC)  # Raiz palabras
library(caret)
library(e1071)
library(plyr)
library(caTools)
tweets = read.csv("prueba.csv", sep = "," , encoding="UTF-8", stringsAsFactors=FALSE)
table(tweets$sentimiento)
tweets = read.csv("prueba.csv", sep = "," , encoding="UTF-8", stringsAsFactors=FALSE)
table(tweets$sentimiento)
corpus = Corpus(VectorSource(tweets$tweet))
print(corpus)
length(corpus)
content(corpus[[12]])
content(corpus[[1500]])
corpus = tm_map(corpus, tolower) # texto a minusculas
corpus = tm_map(corpus, PlainTextDocument) #
corpus <- tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, c(stopwords("es"),"elecciones2017ec"))
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, stemDocument, language="es")
content(corpus[[12]])
content(corpus[[1500]])
frequencies = DocumentTermMatrix(corpus)
frequencies
findFreqTerms(frequencies, lowfreq = 50)
frequencies
sparse <- removeSparseTerms(frequencies, sparse = 0.995)
sparse
findFreqTerms(sparse, lowfreq = 70)
tweetsSparse <- as.data.frame(as.matrix(sparse))
colnames(tweetsSparse) = make.names(colnames(tweetsSparse))
tweetsSparse$sentimiento <- tweets$sentimiento
set.seed(12)
trainSparse = tweetsSparse[1:1720,]
testSparse = tweetsSparse[1721:4645,]
table(testSparse$sentimiento)
SVM <- svm(as.factor(sentimiento)~ ., data = trainSparse)
summary(SVM)
trainSparse
predictSVM <- predict(SVM, newdata = testSparse)
table(predictSVM)
install.packages("tm") # Text Mining
install.packages("SnowballC")
install.packages("caTools")
install.packages("caret")
install.packages("e1071")
install.packages("wordcloud")
Prop=c(3,7,9,1,2)
# Make the default Pie Plot
pie(Prop)
# You can also custom the labels:
pie(Prop , labels = c("Gr-A","Gr-B","Gr-C","Gr-D","Gr-E"))
# If you give a low value to the "edge" argument, you go from something circular to a shape with edges
pie(Prop , labels = c("Gr-A","Gr-B","Gr-C","Gr-D","Gr-E") , edges=10)
# With the radius argument, you choose to zoom in (high values --> big pie) or out (low values --> small pie)
pie(Prop , labels = c("Gr-A","Gr-B","Gr-C","Gr-D","Gr-E") , radius=10)
pie(Prop , labels = c("Gr-A","Gr-B","Gr-C","Gr-D","Gr-E") , radius=0.2)
# The clockwise function to decide where you add the first group. If false, starts 90° right, it true, starts on the top of the pie
pie(Prop , labels = c("Gr-A","Gr-B","Gr-C","Gr-D","Gr-E") , clockwise = FALSE)
pie(Prop , labels = c("Gr-A","Gr-B","Gr-C","Gr-D","Gr-E") , clockwise = TRUE)
# The density arguments permits to add shading lines, and you can control the angle of this lines with "angle"
pie(Prop , labels = c("Gr-A","Gr-B","Gr-C","Gr-D","Gr-E") , density=10 , angle=c(20,90,30,10,0))
# You can change the border of each area with the classical parameters:
pie(Prop , labels = c("Gr-A","Gr-B","Gr-C","Gr-D","Gr-E")  , border="grey" )
Prop
pie(Prop , labels = c("Gr-A","Gr-B","Gr-C","Gr-D","Gr-E")  , border="grey" )
pie(Prop , labels = c("Gr-A","Gr-B","Gr-C","Gr-D","Gr-E") , density=10 , angle=c(20,90,30,10,0))
pie(Prop , labels = c("Gr-A","Gr-B","Gr-C","Gr-D","Gr-E") , clockwise = TRUE)
pie(Prop , labels = c("Gr-A","Gr-B","Gr-C","Gr-D","Gr-E") , clockwise = FALSE)
pie(Prop , labels = c("Gr-A","Gr-B","Gr-C","Gr-D","Gr-E") , radius=0.2)
pie(Prop , labels = c("Gr-A","Gr-B","Gr-C","Gr-D","Gr-E") , radius=10)
pie(Prop , labels = c("Gr-A","Gr-B","Gr-C","Gr-D","Gr-E") , edges=10)
pie(Prop , labels = c("Gr-A","Gr-B","Gr-C","Gr-D","Gr-E") , edges=10)
pie(Prop , labels = c("Gr-A","Gr-B","Gr-C","Gr-D","Gr-E") , edges=10)
pie(Prop , labels = c("Gr-A","Gr-B","Gr-C","Gr-D","Gr-E"))
pie(Prop)
pie(Prop , labels = c("Gr-A","Gr-B","Gr-C","Gr-D","Gr-E"))
pie(Prop , labels = scales::percent)
pie(Prop , labels = c("Gr-A","Gr-B","Gr-C","Gr-D","Gr-E"))+ scale_y_continuous(labels = scales::percent)
# Pie Chart with Percentages
slices <- c(10, 12, 4, 16, 8)
lbls <- c("US", "UK", "Australia", "Germany", "France")
pct <- round(slices/sum(slices)*100)
lbls <- paste(lbls, pct) # add percents to labels
lbls <- paste(lbls,"%",sep="") # ad % to labels
pie(slices,labels = lbls, col=rainbow(length(lbls)),
main="Pie Chart of Countries")
# create a dataset
specie=c(rep("Moreno" , 2) , rep("Lasso" , 2))
condition=rep(c("positivo" , "negativo" ) , 2)
#value=abs(rnorm(4 , 0 , 15))
value=c(758,702,361,344)
data=data.frame(specie,condition,value)
# Grouped
ggplot(data, aes(fill=condition, y=value, x=specie)) +
geom_bar(position="dodge", stat="identity")+
scale_fill_manual(values = c("#BDBDBD","#81F7F3") )
ot
library(ggplot2)
library(ggplot2)
specie=c(rep("Moreno" , 2) , rep("Lasso" , 2))
condition=rep(c("positivo" , "negativo" ) , 2)
value=c(758,702,361,344)
data=data.frame(specie,condition,value)
ggplot(data, aes(fill=condition, y=value, x=specie)) +
geom_bar(position="dodge", stat="identity")+
scale_fill_manual(values = c("#BDBDBD","#81F7F3") )
slices <- c(758, 702, 361, 344)
slices <- c(758, 702, 361, 344)
slices <- c(758, 702, 361, 344)
slices <- c(758, 702, 361, 344)
slices <- c(758, 702, 361, 344)
lbls <- c("POS Moreno", "NEG Moreno", "POS Lasso", "NEG Lasso")
pct <- round(slices/sum(slices)*100)
lbls <- paste(lbls, pct) # add percents to labels
lbls <- paste(lbls,"%",sep="") # ad % to labels
pie(slices,labels = lbls, col=rainbow(length(lbls)),
main="Pie Chart of Countries")
pie(slices,labels = lbls, col=rainbow(length(lbls)),
main="Pie Chart of Countries")
pie(slices,labels = lbls, col=rainbow(length(lbls)),
main="Pie Chart of Countries")
pie(slices,labels = lbls),
main="Pie Chart of Countries")
pie(slices,labels = lbls, col=rainbow(length(lbls)),
main="Pie Chart of Countries")+
scale_fill_manual(values = c("#BDBDBD","#81F7F3") )
pie(slices,labels = lbls,main="Pie Chart of Countries")
pie(slices,labels = lbls, col = c("#BDBDBD","#81F7F3", "#81F7F3","#81F7F3") , main="Pie Chart of Countries")
pie(slices,labels = lbls, col = c("#CEFF33","#3A4C02", "#2057CC","#273C6A") , main="Pie Chart of Countries")
pie(slices,labels = lbls, col = c("#D7E23E","#3A4C02", "#2057CC","#273C6A") , main="Pie Chart of Countries")
pie(slices,labels = lbls, col = c("#D7E23E","#3A4C02", "#053679","#273C6A") , main="Pie Chart of Countries")
pie(slices,labels = lbls, col=rainbow(length(lbls)),main="Porcentaje Comentarios sobre Candidatos")
pie(slices,labels = lbls, col = c("#D7E23E","#3A4C02", "#053679","#273C6A") , main="Porcentaje Comentarios sobre Candidatos")
pie(slices,labels = lbls, col=rainbow(length(lbls)),main="Comentarios POS y NEG sobre Candidatos")
pie(slices,labels = lbls, col = c("#D7E23E","#3A4C02", "#053679","#273C6A") , main="Porcentaje Comentarios sobre Candidatos")
pie(slices,labels = lbls, col = c("#D7E23E","#3A4C02", "#053679","#273C6A") , main="Comentarios sobre Candidatos")
pie(slices,labels = lbls, col = c("#D7E23E","#3A4C02", "#053679","#273C6A") , main="Comentarios sobre Candidatos")
pie(slices,labels = lbls, col = c("#D7E23E","#3A4C02", "#053679","#273C6A") , main="Tweets sobre Candidatos")
# Pie Chart with Percentages
slices <- c(1102, 1063)
lbls <- c("Moreno", "Lasso")
pct <- round(slices/sum(slices)*100)
lbls <- paste(lbls, pct) # add percents to labels
lbls <- paste(lbls,"%",sep="") # ad % to labels
#pie(slices,labels = lbls, col=rainbow(length(lbls)),main="Tweets POS y NEG sobre Candidatos")
pie(slices,labels = lbls, col = c("#D7E23E", "#053679") , main="Tweets sobre Candidatos")
pie(slices,labels = lbls, col = c("#D7E23E", "#053679") , main="Estimación Intención voto")
slices <- c(758, 702, 361, 344)
lbls <- c("POS Moreno", "NEG Moreno", "POS Lasso", "NEG Lasso")
pct <- round(slices/sum(slices)*100)
lbls <- paste(lbls, pct) # add percents to labels
lbls <- paste(lbls,"%",sep="") # ad % to labels
#pie(slices,labels = lbls, col=rainbow(length(lbls)),main="Tweets POS y NEG sobre Candidatos")
pie(slices,labels = lbls, col = c("#D7E23E","#3A4C02", "#053679","#273C6A") , main="Tweets POS y NEG sobre Candidatos")
slices <- c(1102, 1063)
lbls <- c("Moreno", "Lasso")
pct <- round(slices/sum(slices)*100)
lbls <- paste(lbls, pct) # add percents to labels
lbls <- paste(lbls,"%",sep="") # ad % to labels
#pie(slices,labels = lbls, col=rainbow(length(lbls)),main="Tweets POS y NEG sobre Candidatos")
pie(slices,labels = lbls, col = c("#D7E23E", "#053679") , main="Estimación Intención voto")
pie(slices,labels = lbls, col = c("#D7E23E", "#053679") , main="Estimación Intención voto")
specie=c(rep("Moreno" , 2) , rep("Lasso" , 2))
condition=rep(c("positivo" , "negativo" ) , 2)
#value=abs(rnorm(4 , 0 , 15))
value=c(758,702,361,344)
data=data.frame(specie,condition,value)
# Grouped
ggplot(data, aes(fill=condition, y=value, x=specie)) +
geom_bar(position="dodge", stat="identity")+
scale_fill_manual(values = c("#BDBDBD","#81F7F3") )
